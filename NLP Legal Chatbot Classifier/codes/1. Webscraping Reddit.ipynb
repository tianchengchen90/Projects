{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2d0701",
   "metadata": {},
   "source": [
    "# Project 3 Part 1: Webscraping Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c89b5",
   "metadata": {},
   "source": [
    "This workbook is 1 out of 3 parts of an Natural Language Processing (NLP) classification model. The focus of this part is to gather the necessary data from Reddit for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ecd23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc4146",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d8385",
   "metadata": {},
   "source": [
    "The use of chatbots and other bots are increasingly prevalent in modern times. Corporations, and government bodies alike make use of these bots to handle repetitive tasks. Some of the chatbots rely on automated responses, while others are using Machine Learning to enable better responses that are closer to human interaction. In order to make the interaction more immersive, there has been a trend towards machine learning models for such chatbot. \n",
    "\n",
    "With this growing trend, legal implications to organisations may arise, since there has been several instances of such chatbots misbehaving and giving harmful replies, ranging from defamation, to telling people to kill their parents. Machine Learning models may inadvertantly dish out legal advices that makes the company which owns the chatbot liable if they do not work out.\n",
    "\n",
    "In light of this trend, it may be beneficial for Machine Learning model to classify what is a query that is asking for legal advice and what is a query that is not legal advice. The response when a legal query is received can be to direct it to the relevant authorities, caveat and protect the owner of the bot before answering, or just not responding to those types of queries.\n",
    "\n",
    "I have opted to use `r/NoStupidQuestions` to mimic real life questions, where users from all walks of life ask any questions they want, while `r/legaladvice` to mimic real life legal queries and questions that have legal implications. This project will look at how successful I will be able to correctly classify posts from the two subreddits.\n",
    "\n",
    "It is interesting to note that `r/NoStupidQuestions` contains questions that have no boundaries, since all questions are welcome. The result is a melting pot of questions which mimics the borderline nonsensical questions that many asks chatbots out of leisurely purposes, as well as serious and insightful questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b104846",
   "metadata": {},
   "source": [
    "## Reddit Pushshift API scraping\n",
    "I wrote a while loop to enable me to extract the important fields from each subreddit, ensuring that I get 12,000 posts from each of the subreddits of choice. To ensure that I do not go beyond the Pushshift API's hashrate of 60 request per second, I will sleep the code for 2 seconds at the end of each line. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5cf00a",
   "metadata": {},
   "source": [
    "## Code for extracting `r/NoStupidQuestions` submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022a05b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initiate date to search\n",
    "date = 1647752175\n",
    "\n",
    "# Instatiate main df for nostupidquestions\n",
    "df_nostupidquestions = pd.DataFrame(columns = ['author',\n",
    "                                               'is_self',\n",
    "                                               'is_video',\n",
    "                                               'num_comments',\n",
    "                                               'permalink',\n",
    "                                               'score',\n",
    "                                               'selftext',\n",
    "                                               'subreddit', \n",
    "                                               'title', \n",
    "                                               'upvote_ratio',\n",
    "                                               'created_utc'\n",
    "                                              ])\n",
    "\n",
    "\n",
    "while df_nostupidquestions.shape[0]<12000:\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params ={\n",
    "        'subreddit': 'NoStupidQuestions',\n",
    "        'before': date,\n",
    "        'size': 100\n",
    "    }\n",
    "    # create temporary df \n",
    "    data_temp = pd.DataFrame(requests.get(url,params).json()['data'])[['author',\n",
    "                                               'is_self',\n",
    "                                               'is_video',\n",
    "                                               'num_comments',\n",
    "                                               'permalink',\n",
    "                                               'score',\n",
    "                                               'selftext',                        \n",
    "                                               'subreddit', \n",
    "                                               'title', \n",
    "                                               'upvote_ratio',\n",
    "                                               'created_utc'\n",
    "                                              ]]\n",
    "\n",
    "    # create list for index with [removed] and '' \n",
    "    index_df_nostupidquestions = list(data_temp[(data_temp['selftext'] == '[removed]') | (data_temp['selftext'] == '')].index)\n",
    "\n",
    "    # drop down all in index list \n",
    "    data_temp = data_temp.drop(index = index_df_nostupidquestions)\n",
    "\n",
    "    # concat with main df\n",
    "    df_nostupidquestions = pd.concat([df_nostupidquestions,data_temp])\n",
    "\n",
    "    # print to check on shape \n",
    "    print(df_nostupidquestions.shape)\n",
    "\n",
    "    # set up new date for searching after \n",
    "    date = df_nostupidquestions.iloc[-1,-1]\n",
    "    \n",
    "    #sleep 2 sec to allow within 60 request per second\n",
    "    time.sleep(2)\n",
    "\n",
    "df_nostupidquestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64d38e",
   "metadata": {},
   "source": [
    "## Code for extracting `r/legaladvice` submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235d6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate date to search\n",
    "date = 1647752175\n",
    "\n",
    "#instatiate main df for legaladvice\n",
    "df_legaladvice = pd.DataFrame(columns = ['author',\n",
    "                                               'is_self',\n",
    "                                               'is_video',\n",
    "                                               'num_comments',\n",
    "                                               'permalink',\n",
    "                                               'score',\n",
    "                                               'selftext',\n",
    "                                               'subreddit', \n",
    "                                               'title', \n",
    "                                               'upvote_ratio',\n",
    "                                               'created_utc'\n",
    "                                              ])\n",
    "\n",
    "\n",
    "while df_legaladvice.shape[0]<12000:\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params ={\n",
    "        'subreddit': 'legaladvice',\n",
    "        'before': date,\n",
    "        'size': 100\n",
    "    }\n",
    "    # create temporary df \n",
    "    data_temp = pd.DataFrame(requests.get(url,params).json()['data'])[['author',\n",
    "                                               'is_self',\n",
    "                                               'is_video',\n",
    "                                               'num_comments',\n",
    "                                               'permalink',\n",
    "                                               'score',\n",
    "                                               'selftext',                        \n",
    "                                               'subreddit', \n",
    "                                               'title', \n",
    "                                               'upvote_ratio',\n",
    "                                               'created_utc'\n",
    "                                              ]]\n",
    "\n",
    "    # create list for index with [removed] and '' \n",
    "    index_df_legaladvice = list(data_temp[(data_temp['selftext'] == '[removed]') | (data_temp['selftext'] == '')].index)\n",
    "\n",
    "    # drop down all in index list \n",
    "    data_temp = data_temp.drop(index = index_df_legaladvice)\n",
    "\n",
    "    # concat with main df\n",
    "    df_legaladvice = pd.concat([df_legaladvice,data_temp])\n",
    "\n",
    "    # print to check on shape \n",
    "    print(df_legaladvice.shape)\n",
    "\n",
    "    # set up new date for searching after \n",
    "    date = df_legaladvice.iloc[-1,-1]\n",
    "    \n",
    "    #sleep 2 sec to allow within 60 request per second\n",
    "    time.sleep(2)\n",
    "\n",
    "df_legaladvice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b2dd0",
   "metadata": {},
   "source": [
    "## Save dataframes to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec3e0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nostupidquestions.to_csv('../datasets/nostupid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82fb837",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_legaladvice.to_csv('../datasets/legaladvice.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
